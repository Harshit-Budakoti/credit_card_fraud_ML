# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zD8rilWmjz0w3b7EmU2UohINKIyViUkE
"""
from dagshub.streaming import install_hooks
import os

# Fetch the DagsHub API key from environment variables
dagshub_api_key = os.getenv('DAGSHUB_ACCESS_KEY')

install_hooks(repo_url='https://dagshub.com/harshitbudakotimatsc20/credit_card_fraud_ML.git', token=dagshub_api_key)

from pyspark.sql import SparkSession

# Initialize a SparkSession
spark = SparkSession.builder.getOrCreate()
import os

dataset1_path = os.getenv('DATASET3_PATH') #s3://credit_card_fraud_ML/processed_old_data.parquet
dataset2_path = os.getenv('DATASET4_PATH')  #s3://credit_card_fraud_ML/processed_new_data.parquet

# Specify the path to the CSV file in your DagsHub repository
file_path1 = dataset1_path   #'/content/credit_card_fraud_classifier/timetraindata.csv'
file_path2 =dataset2_path # '/content/credit_card_fraud_classifier/raw_new_dataframe.csv'
# Read the CSV file
df1 = spark.read.parquet(file_path1, inferSchema=True, header=True)
df2 = spark.read.parquet(file_path2.inferSchema = True,header = True)
import pyspark
req_columns = ['gender', 'job', 'profile', 'category', 'merchant']
def ordinal_encoding_test(data,testdata,req_col):
    from pyspark.sql.functions import col
    tmp_data = data.groupby(req_col).agg({'is_fraud':'sum'}).withColumnRenamed("sum(is_fraud)","occur")
    tmp_data = tmp_data.orderBy('occur')
    from pyspark.sql import Window
    from pyspark.sql.functions import row_number,dense_rank
    # Define a window specification
    windowSpec = Window.orderBy("occur")  # replace "some_column" with a column name
    # Add row numbers
    tmp_data2 = tmp_data.withColumn("rnk", dense_rank().over(windowSpec))
    tmp_data2 = tmp_data2.withColumn("rnk", col("rnk").cast("string"))
    test_keys2 = tmp_data2.select(req_col,'rnk').toPandas()[req_col].tolist()
    test_values2 = tmp_data2.select(req_col,'rnk').toPandas()['rnk'].tolist()
    replacements2 = dict(zip(test_keys2, test_values2))
    data = data.replace(to_replace=replacements2, subset=[req_col])
    data = data.withColumn(req_col, col(req_col).cast("integer"))
    testdata = testdata.replace(to_replace=replacements2, subset=[req_col])
    testdata = testdata.withColumn(req_col, col(req_col).cast("integer"))
    return data,testdata
for i in req_columns:
    traindf,testdf = ordinal_encoding_test(traindf,testdf,i)
        
        
traindf.write.mode('overwrite').parquet('encoded_past_data.parquet')
testdf.write.mode('overwrite').parquet('encoded_new_data.parquet')

from dagshub import get_repo_bucket_client
s3 = get_repo_bucket_client("harshitbudakotimatsc20/credit_card_fraud_ML")

# Upload file
s3.upload_file(
    Bucket="credit_card_fraud_ML",  # name of the repo
    Filename="encoded_past_data.parquet",  # local path of file to upload
    Key="encoded_past_data.parquet"  # remote path where to upload the file
)
s3.upload_file(
    Bucket="credit_card_fraud_ML",  # name of the repo
    Filename="encoded_new_data.parquet",  # local path of file to upload
    Key="encoded_new_data.parquet"  # remote path where to upload the file
)
spark.stop()


    
