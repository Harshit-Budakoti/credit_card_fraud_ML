# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zD8rilWmjz0w3b7EmU2UohINKIyViUkE
"""
from dagshub.streaming import install_hooks
install_hooks()

import os
import json

# Get the path to the config file from the environment variable
config_path = os.getenv('CONFIG_PATH')

# Load hyperparameters
with open(config_path) as f:
    params = json.load(f)

dataset6_path = os.getenv('DATASET6_PATH')
dataset7_path = os.getenv('DATASET7_PATH')

from pyspark.sql import SparkSession

# Initialize a SparkSession
spark = SparkSession.builder.getOrCreate() 
file_path6 = dataset6_path #s3://credit_card_fraud_ML/model_traindata.parquet

# Read the CSV file
df = spark.read.parquet(dataset6_path, inferSchema=True, header=True)
df7 = spark.read.parquet(dataset7_path, inferSchema=True, header=True)
traincols = ['ssn','gender','zip','lat','long','city_pop','job','acct_num','profile','trans_date','category','amt','is_fraud','merchant','merch_lat','merch_long','age_at_time_of_transaction',
 'day','hour','month']

from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from xgboost.spark import SparkXGBClassifier
# Assuming df is my model training  DataFrame and "is_fraud" is the column I want to predict
# Assemble all features into a single vector
assembler = VectorAssembler(inputCols = train_cols, outputCol="features")
# Define the classifier

xgb_class = SparkXGBClassifier(
  features_col="features",
  label_col="is_fraud",
  num_workers=1,
  **params
)
# Chain indexers, assembler and forest in a Pipeline
pipeline_xgb_new = Pipeline(stages=[assembler, xgb_class])
xgb_time_model = pipeline_xgb_new.fit(new_traindf)

testdatapred = xgb_time_model.transform(df7)
from pyspark.ml.evaluation import BinaryClassificationEvaluator
evaluatorauc = BinaryClassificationEvaluator(labelCol="is_fraud", rawPredictionCol="prediction", metricName="areaUnderROC")
auc = evaluatorauc.evaluate(testdatapred)
pos= predtr[predtr.is_fraud ==1.0]
neg= predtr[predtr.is_fraud ==0.0]
tp = pos[pos.prediction == 1.0].count()
fn = pos[pos.prediction == 0.0].count()
fp = neg[neg.prediction == 1.0].count()
tn = neg.count()- fp
recall = tp/(tp+fn)
precision = tp/(tp+fp)
fprate = fp/(fp+tn)
accuracy= (tp+tn)/(tp+tn+fp+fn)
fscore = 2*precision*recall/(precision+recall)
with mlflow.start_run():
    for param, value in params.items():
        mlflow.log_param(param, value)
    mlflow.log_metric("AUC",auc)
    mlflow.log_metric("accuracy",accuracy)
    mlflow.log_metric("precision",precision)
    mlflow.log_metric("recall",recall)
    mlflow.log_metric("Fscore",fscore)
    mlflow.log_metric("False_positivity_rate",fprate)
    # Log the model
    mlflow.spark.log_model(xgb_time_model, "CI_CD_PIPELINE_XGBOOST_PERFECT_MODEL")


from dagshub import get_repo_bucket_client
testdatapred.write.mode('overwrite').parquet('latest_predictions.parquet')
s3 = get_repo_bucket_client("harshitbudakotimatsc20/credit_card_fraud_ML")

# Upload file
s3.upload_file(
    Bucket="credit_card_fraud_ML",  # name of the repo
    Filename="latest_predictions.parquet",  # local path of file to upload
    Key="latest_predictions.parquet"  # remote path where to upload the file
)


spark.stop()

